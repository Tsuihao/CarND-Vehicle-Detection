{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_scientist import merge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars contain 8792 images\n",
      "nocars contain 8968 images\n"
     ]
    }
   ],
   "source": [
    "cars_path, nocars_path = merge_dataset()\n",
    "print('cars contain {} images'.format(len(cars_path)))\n",
    "print('nocars contain {} images'.format(len(nocars_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['color_space'] = 'HLS'   # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "config['orient'] = 9              # HOG orientations\n",
    "config['pix_per_cell'] = 8        # HOG pixels per cell\n",
    "config['cell_per_block'] = 2      # HOG cells per block\n",
    "config['hog_channel'] = 'ALL'     # Can be 0, 1, 2, or \"ALL\"\n",
    "config['spatial_size'] = (16, 16) # Spatial binning dimensions\n",
    "config['hist_bins'] = 32          # Number of histogram bins\n",
    "config['spatial_feat'] = True     # Spatial features on or off\n",
    "config['hist_feat'] = True        # Histogram features on or off\n",
    "config['hog_feat'] = True         # HOG features on or off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = config['color_space']\n",
    "orient = config['orient']\n",
    "pix_per_cell = config['pix_per_cell']\n",
    "cell_per_block = config['cell_per_block']\n",
    "hog_channel = config['hog_channel']\n",
    "spatial_size = config['spatial_size']\n",
    "hist_bins = config['hist_bins']\n",
    "spatial_feat = config['spatial_feat']\n",
    "hist_feat = config['hist_feat']\n",
    "hog_feat = config['hog_feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lesson_functions import extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 cells below only need to execute once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note here that when you are using a scaler to train a classifier, you want to only **fit** the scaler on the **training data**, and then **transform both the training and test sets using the scaler**. Why? If you provide both the training and test set to the scaler, you are allowing your model a peek into the values contained in the test set, and it's no longer as useful at generalizing to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "car_features = extract_features(cars_path, color_space=color_space, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                                hog_channel=hog_channel, spatial_size=spatial_size, hist_bins=hist_bins, spatial_feat=spatial_feat, hist_feat=hist_feat,\n",
    "                                hog_feat=hog_feat)\n",
    "nocar_features = extract_features(nocars_path, color_space=color_space, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                                hog_channel=hog_channel, spatial_size=spatial_size, hist_bins=hist_bins, spatial_feat=spatial_feat, hist_feat=hist_feat,\n",
    "                                hog_feat=hog_feat)\n",
    "\n",
    "\n",
    "features = np.vstack((car_features, nocar_features)).astype(np.float64)\n",
    "labels = np.hstack((np.ones(len(cars_path)), np.zeros(len(nocars_path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit(features)\n",
    "scaled_features = Xscaler.transform(features)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(scaled_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit from the training data\n",
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit(X_train)\n",
    "\n",
    "# Transform both training and test data\n",
    "X_train_scaled = Xscaler.transform(X_train)\n",
    "X_valid_scaled = Xscaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "using SVM as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is 0.9901463963963963 %\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "print('The validation accuracy is {} %'.format((svc.score(X_valid_scaled, y_valid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier saved to ./trained_models/SVM_HLS_1.p\n"
     ]
    }
   ],
   "source": [
    "from vehicle_classifier import save_trained_model\n",
    "save_trained_model('./trained_models/SVM_HLS_1.p', svc, Xscaler, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
